{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "religious-facility",
   "metadata": {},
   "source": [
    "# 불균형 데이터 분류\n",
    "\n",
    "# Imbalanced classification : credit card fraud detection  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-denial",
   "metadata": {},
   "source": [
    "한 클래스의 데이터 수가 다른 클래스보다 훨씬 많은 경우를 __불균형한 데이터 (Imbalanced classification)__ 라고 한다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-scratch",
   "metadata": {},
   "source": [
    "이러한 불균형 데이터셋을 분류해보자.  \n",
    "데이터는 kaggle의 [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud) 을 불러와 사용한다.\n",
    "\n",
    "[Tensorflow : 불균형 데이터 분류](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#%ED%9B%88%EB%A0%A8_%EC%9D%B4%EB%A0%A5_%EC%9E%AC%ED%99%95%EC%9D%B8)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/84179578/127277084-5697d07b-a2df-444e-af5d-45553ed254b0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-contrary",
   "metadata": {},
   "source": [
    "## 필요한 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "american-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-silly",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hispanic-survivor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADER: \"Time\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\",\"Amount\",\"Class\"\n",
      "features.shape: (284807, 30)\n",
      "targets.shape: (284807, 1)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "fname = \"data/creditcard.csv\"\n",
    "\n",
    "all_features = []\n",
    "all_targets = []\n",
    "with open(fname) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            print(\"HEADER:\", line.strip())\n",
    "            continue                        # 컬럼 이름은 제외\n",
    "        fields = line.strip().split(\",\")\n",
    "        all_features.append([float(v.replace('\"', \"\")) for v in fields[:-1]])\n",
    "        all_targets.append([int(fields[-1].replace('\"', \"\"))])\n",
    "\n",
    "\n",
    "features = np.array(all_features, dtype=\"float32\")\n",
    "targets = np.array(all_targets, dtype=\"uint8\")\n",
    "print(\"features.shape:\", features.shape)\n",
    "print(\"targets.shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-train",
   "metadata": {},
   "source": [
    "## training set 과 validation set 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unauthorized-venezuela",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 227846\n",
      "Number of validation samples: 56961\n"
     ]
    }
   ],
   "source": [
    "num_val_samples = int(len(features) * 0.2)\n",
    "train_features = features[:-num_val_samples]\n",
    "train_targets = targets[:-num_val_samples]\n",
    "val_features = features[-num_val_samples:]\n",
    "val_targets = targets[-num_val_samples:]\n",
    "\n",
    "print(\"Number of training samples:\", len(train_features))\n",
    "print(\"Number of validation samples:\", len(val_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-responsibility",
   "metadata": {},
   "source": [
    "## 불균형 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aboriginal-wellington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples in training data: 417 (0.18% of total)\n"
     ]
    }
   ],
   "source": [
    "counts = np.bincount(train_targets[:, 0])\n",
    "print(\n",
    "    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "        counts[1], 100 * float(counts[1]) / len(train_targets)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-separate",
   "metadata": {},
   "source": [
    "## 정규화 Normalize\n",
    "\n",
    "train set 의 평균과 표준편차를 이용해 정규화 과정 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "limited-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(train_features, axis=0)\n",
    "train_features -= mean\n",
    "val_features -= mean\n",
    "std = np.std(train_features, axis=0)\n",
    "train_features /= std\n",
    "val_features /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-explosion",
   "metadata": {},
   "source": [
    "## binary classification 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "false-shower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               7936      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 139,777\n",
      "Trainable params: 139,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(\n",
    "            256, activation=\"relu\", input_shape=(train_features.shape[-1],)\n",
    "        ),\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-friday",
   "metadata": {},
   "source": [
    "## class_weight 인자를 이용해 모델 학습  --> KET POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "seasonal-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_for_0 = 1.0 / counts[0]    # 가중치를 정하는 수식은 데이터에 따라 유동성있음\n",
    "weight_for_1 = 1.0 / counts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "heated-batch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "112/112 - 3s - loss: 1.5875e-07 - fn: 2.0000 - fp: 1749.0000 - tn: 225680.0000 - tp: 415.0000 - precision: 0.1918 - recall: 0.9952 - val_loss: 0.0103 - val_fn: 13.0000 - val_fp: 193.0000 - val_tn: 56693.0000 - val_tp: 62.0000 - val_precision: 0.2431 - val_recall: 0.8267\n",
      "Epoch 2/30\n",
      "112/112 - 1s - loss: 1.0042e-07 - fn: 0.0000e+00 - fp: 1077.0000 - tn: 226352.0000 - tp: 417.0000 - precision: 0.2791 - recall: 1.0000 - val_loss: 0.0089 - val_fn: 14.0000 - val_fp: 139.0000 - val_tn: 56747.0000 - val_tp: 61.0000 - val_precision: 0.3050 - val_recall: 0.8133\n",
      "Epoch 3/30\n",
      "112/112 - 1s - loss: 1.3670e-07 - fn: 2.0000 - fp: 1151.0000 - tn: 226278.0000 - tp: 415.0000 - precision: 0.2650 - recall: 0.9952 - val_loss: 0.0257 - val_fn: 11.0000 - val_fp: 436.0000 - val_tn: 56450.0000 - val_tp: 64.0000 - val_precision: 0.1280 - val_recall: 0.8533\n",
      "Epoch 4/30\n",
      "112/112 - 1s - loss: 1.2207e-07 - fn: 0.0000e+00 - fp: 1443.0000 - tn: 225986.0000 - tp: 417.0000 - precision: 0.2242 - recall: 1.0000 - val_loss: 0.0104 - val_fn: 13.0000 - val_fp: 152.0000 - val_tn: 56734.0000 - val_tp: 62.0000 - val_precision: 0.2897 - val_recall: 0.8267\n",
      "Epoch 5/30\n",
      "112/112 - 1s - loss: 9.2090e-08 - fn: 0.0000e+00 - fp: 995.0000 - tn: 226434.0000 - tp: 417.0000 - precision: 0.2953 - recall: 1.0000 - val_loss: 0.0134 - val_fn: 14.0000 - val_fp: 202.0000 - val_tn: 56684.0000 - val_tp: 61.0000 - val_precision: 0.2319 - val_recall: 0.8133\n",
      "Epoch 6/30\n",
      "112/112 - 1s - loss: 8.3564e-08 - fn: 0.0000e+00 - fp: 867.0000 - tn: 226562.0000 - tp: 417.0000 - precision: 0.3248 - recall: 1.0000 - val_loss: 0.0094 - val_fn: 14.0000 - val_fp: 110.0000 - val_tn: 56776.0000 - val_tp: 61.0000 - val_precision: 0.3567 - val_recall: 0.8133\n",
      "Epoch 7/30\n",
      "112/112 - 1s - loss: 8.6271e-08 - fn: 0.0000e+00 - fp: 760.0000 - tn: 226669.0000 - tp: 417.0000 - precision: 0.3543 - recall: 1.0000 - val_loss: 0.0110 - val_fn: 14.0000 - val_fp: 129.0000 - val_tn: 56757.0000 - val_tp: 61.0000 - val_precision: 0.3211 - val_recall: 0.8133\n",
      "Epoch 8/30\n",
      "112/112 - 1s - loss: 5.8740e-07 - fn: 6.0000 - fp: 4033.0000 - tn: 223396.0000 - tp: 411.0000 - precision: 0.0925 - recall: 0.9856 - val_loss: 0.0538 - val_fn: 8.0000 - val_fp: 1081.0000 - val_tn: 55805.0000 - val_tp: 67.0000 - val_precision: 0.0584 - val_recall: 0.8933\n",
      "Epoch 9/30\n",
      "112/112 - 1s - loss: 3.5654e-07 - fn: 1.0000 - fp: 4726.0000 - tn: 222703.0000 - tp: 416.0000 - precision: 0.0809 - recall: 0.9976 - val_loss: 0.0216 - val_fn: 9.0000 - val_fp: 470.0000 - val_tn: 56416.0000 - val_tp: 66.0000 - val_precision: 0.1231 - val_recall: 0.8800\n",
      "Epoch 10/30\n",
      "112/112 - 1s - loss: 3.2451e-07 - fn: 2.0000 - fp: 2915.0000 - tn: 224514.0000 - tp: 415.0000 - precision: 0.1246 - recall: 0.9952 - val_loss: 0.0075 - val_fn: 14.0000 - val_fp: 132.0000 - val_tn: 56754.0000 - val_tp: 61.0000 - val_precision: 0.3161 - val_recall: 0.8133\n",
      "Epoch 11/30\n",
      "112/112 - 1s - loss: 1.5817e-07 - fn: 0.0000e+00 - fp: 1927.0000 - tn: 225502.0000 - tp: 417.0000 - precision: 0.1779 - recall: 1.0000 - val_loss: 0.0135 - val_fn: 10.0000 - val_fp: 233.0000 - val_tn: 56653.0000 - val_tp: 65.0000 - val_precision: 0.2181 - val_recall: 0.8667\n",
      "Epoch 12/30\n",
      "112/112 - 1s - loss: 1.9198e-07 - fn: 1.0000 - fp: 2123.0000 - tn: 225306.0000 - tp: 416.0000 - precision: 0.1638 - recall: 0.9976 - val_loss: 0.0116 - val_fn: 12.0000 - val_fp: 198.0000 - val_tn: 56688.0000 - val_tp: 63.0000 - val_precision: 0.2414 - val_recall: 0.8400\n",
      "Epoch 13/30\n",
      "112/112 - 1s - loss: 2.7901e-07 - fn: 3.0000 - fp: 2813.0000 - tn: 224616.0000 - tp: 414.0000 - precision: 0.1283 - recall: 0.9928 - val_loss: 0.0304 - val_fn: 8.0000 - val_fp: 692.0000 - val_tn: 56194.0000 - val_tp: 67.0000 - val_precision: 0.0883 - val_recall: 0.8933\n",
      "Epoch 14/30\n",
      "112/112 - 1s - loss: 1.4933e-07 - fn: 1.0000 - fp: 1811.0000 - tn: 225618.0000 - tp: 416.0000 - precision: 0.1868 - recall: 0.9976 - val_loss: 0.0153 - val_fn: 12.0000 - val_fp: 295.0000 - val_tn: 56591.0000 - val_tp: 63.0000 - val_precision: 0.1760 - val_recall: 0.8400\n",
      "Epoch 15/30\n",
      "112/112 - 1s - loss: 1.1766e-07 - fn: 0.0000e+00 - fp: 1289.0000 - tn: 226140.0000 - tp: 417.0000 - precision: 0.2444 - recall: 1.0000 - val_loss: 0.0165 - val_fn: 12.0000 - val_fp: 256.0000 - val_tn: 56630.0000 - val_tp: 63.0000 - val_precision: 0.1975 - val_recall: 0.8400\n",
      "Epoch 16/30\n",
      "112/112 - 1s - loss: 1.6559e-07 - fn: 2.0000 - fp: 1569.0000 - tn: 225860.0000 - tp: 415.0000 - precision: 0.2092 - recall: 0.9952 - val_loss: 0.0352 - val_fn: 11.0000 - val_fp: 542.0000 - val_tn: 56344.0000 - val_tp: 64.0000 - val_precision: 0.1056 - val_recall: 0.8533\n",
      "Epoch 17/30\n",
      "112/112 - 1s - loss: 2.5238e-07 - fn: 3.0000 - fp: 2045.0000 - tn: 225384.0000 - tp: 414.0000 - precision: 0.1684 - recall: 0.9928 - val_loss: 0.0949 - val_fn: 10.0000 - val_fp: 718.0000 - val_tn: 56168.0000 - val_tp: 65.0000 - val_precision: 0.0830 - val_recall: 0.8667\n",
      "Epoch 18/30\n",
      "112/112 - 1s - loss: 2.8601e-07 - fn: 2.0000 - fp: 2904.0000 - tn: 224525.0000 - tp: 415.0000 - precision: 0.1250 - recall: 0.9952 - val_loss: 0.0111 - val_fn: 12.0000 - val_fp: 202.0000 - val_tn: 56684.0000 - val_tp: 63.0000 - val_precision: 0.2377 - val_recall: 0.8400\n",
      "Epoch 19/30\n",
      "112/112 - 1s - loss: 1.3018e-07 - fn: 0.0000e+00 - fp: 1392.0000 - tn: 226037.0000 - tp: 417.0000 - precision: 0.2305 - recall: 1.0000 - val_loss: 0.0131 - val_fn: 13.0000 - val_fp: 237.0000 - val_tn: 56649.0000 - val_tp: 62.0000 - val_precision: 0.2074 - val_recall: 0.8267\n",
      "Epoch 20/30\n",
      "112/112 - 1s - loss: 1.0379e-07 - fn: 0.0000e+00 - fp: 1170.0000 - tn: 226259.0000 - tp: 417.0000 - precision: 0.2628 - recall: 1.0000 - val_loss: 0.0105 - val_fn: 13.0000 - val_fp: 139.0000 - val_tn: 56747.0000 - val_tp: 62.0000 - val_precision: 0.3085 - val_recall: 0.8267\n",
      "Epoch 21/30\n",
      "112/112 - 1s - loss: 1.3107e-07 - fn: 1.0000 - fp: 1508.0000 - tn: 225921.0000 - tp: 416.0000 - precision: 0.2162 - recall: 0.9976 - val_loss: 0.0091 - val_fn: 13.0000 - val_fp: 130.0000 - val_tn: 56756.0000 - val_tp: 62.0000 - val_precision: 0.3229 - val_recall: 0.8267\n",
      "Epoch 22/30\n",
      "112/112 - 1s - loss: 1.4167e-07 - fn: 2.0000 - fp: 1319.0000 - tn: 226110.0000 - tp: 415.0000 - precision: 0.2393 - recall: 0.9952 - val_loss: 0.0084 - val_fn: 14.0000 - val_fp: 88.0000 - val_tn: 56798.0000 - val_tp: 61.0000 - val_precision: 0.4094 - val_recall: 0.8133\n",
      "Epoch 23/30\n",
      "112/112 - 1s - loss: 8.5839e-08 - fn: 0.0000e+00 - fp: 1017.0000 - tn: 226412.0000 - tp: 417.0000 - precision: 0.2908 - recall: 1.0000 - val_loss: 0.0098 - val_fn: 14.0000 - val_fp: 96.0000 - val_tn: 56790.0000 - val_tp: 61.0000 - val_precision: 0.3885 - val_recall: 0.8133\n",
      "Epoch 24/30\n",
      "112/112 - 1s - loss: 2.4320e-07 - fn: 3.0000 - fp: 2122.0000 - tn: 225307.0000 - tp: 414.0000 - precision: 0.1632 - recall: 0.9928 - val_loss: 0.0227 - val_fn: 10.0000 - val_fp: 458.0000 - val_tn: 56428.0000 - val_tp: 65.0000 - val_precision: 0.1243 - val_recall: 0.8667\n",
      "Epoch 25/30\n",
      "112/112 - 1s - loss: 1.6273e-07 - fn: 1.0000 - fp: 2075.0000 - tn: 225354.0000 - tp: 416.0000 - precision: 0.1670 - recall: 0.9976 - val_loss: 0.0222 - val_fn: 9.0000 - val_fp: 388.0000 - val_tn: 56498.0000 - val_tp: 66.0000 - val_precision: 0.1454 - val_recall: 0.8800\n",
      "Epoch 26/30\n",
      "112/112 - 1s - loss: 1.9074e-07 - fn: 1.0000 - fp: 2367.0000 - tn: 225062.0000 - tp: 416.0000 - precision: 0.1495 - recall: 0.9976 - val_loss: 0.0243 - val_fn: 9.0000 - val_fp: 479.0000 - val_tn: 56407.0000 - val_tp: 66.0000 - val_precision: 0.1211 - val_recall: 0.8800\n",
      "Epoch 27/30\n",
      "112/112 - 1s - loss: 9.4464e-08 - fn: 0.0000e+00 - fp: 1070.0000 - tn: 226359.0000 - tp: 417.0000 - precision: 0.2804 - recall: 1.0000 - val_loss: 0.0181 - val_fn: 10.0000 - val_fp: 298.0000 - val_tn: 56588.0000 - val_tp: 65.0000 - val_precision: 0.1791 - val_recall: 0.8667\n",
      "Epoch 28/30\n",
      "112/112 - 1s - loss: 1.0137e-07 - fn: 0.0000e+00 - fp: 1236.0000 - tn: 226193.0000 - tp: 417.0000 - precision: 0.2523 - recall: 1.0000 - val_loss: 0.0122 - val_fn: 11.0000 - val_fp: 194.0000 - val_tn: 56692.0000 - val_tp: 64.0000 - val_precision: 0.2481 - val_recall: 0.8533\n",
      "Epoch 29/30\n",
      "112/112 - 1s - loss: 9.0464e-08 - fn: 0.0000e+00 - fp: 1060.0000 - tn: 226369.0000 - tp: 417.0000 - precision: 0.2823 - recall: 1.0000 - val_loss: 0.0153 - val_fn: 11.0000 - val_fp: 265.0000 - val_tn: 56621.0000 - val_tp: 64.0000 - val_precision: 0.1945 - val_recall: 0.8533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "112/112 - 1s - loss: 1.0274e-07 - fn: 1.0000 - fp: 745.0000 - tn: 226684.0000 - tp: 416.0000 - precision: 0.3583 - recall: 0.9976 - val_loss: 0.0249 - val_fn: 9.0000 - val_fp: 312.0000 - val_tn: 56574.0000 - val_tp: 66.0000 - val_precision: 0.1746 - val_recall: 0.8800\n"
     ]
    }
   ],
   "source": [
    "metrics = [\n",
    "    keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "    keras.metrics.FalsePositives(name=\"fp\"),\n",
    "    keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "    keras.metrics.TruePositives(name=\"tp\"),\n",
    "    keras.metrics.Precision(name=\"precision\"),\n",
    "    keras.metrics.Recall(name=\"recall\"),\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-2), loss=\"binary_crossentropy\", metrics=metrics\n",
    ")\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"fraud_model_at_epoch_{epoch}.h5\")]\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "model.fit(\n",
    "    train_features,\n",
    "    train_targets,\n",
    "    batch_size=2048,\n",
    "    epochs=30,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(val_features, val_targets),\n",
    "    class_weight=class_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-handling",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "불균형한 데이터를 다룰때, 적은 데이터에 더 큰 중요도를 줘야하는 경우가 있다. 이럴때는 모델을 학습시킬때 `class_weight` 인자를 이용하여 가중치를 줄 수 있다.  \n",
    "\n",
    "또한 데이터 준비 단계에서 데이터 수가 적은 샘플에 대해서 __오버샘플링__ 과정을 진행 함으로써 여러 불균형한 데이터셋을 다룰 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-bradford",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
